{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  **Final Assignment**\n",
    "\n",
    "The final assignment intends to summarize and compare the concepts and methods taught throughout the course. We will use a\n",
    " very simple task where the ground truth (optimal policies and value functions) are easy to establish and we will\n",
    " implement and evaluate the performance of some of the most important algorithms in solving the MDP. This task is called `Circle World`.\n",
    "\n",
    "# Circle world\n",
    "\n",
    "Consider an agent navigating a *circular environment*, consisting of a simple circular track divided into `n_states`.\n",
    " From any state, the agent may choose to step `left` or `right`. All actions lead to a reward $-1/(N-1)$, where $N=$ n_states -1, except for\n",
    " actions that lead the agent to the target state 0 which receive a reward of +1. States 0 and $N$ are connected to\n",
    " form a circle and the agent is randomly placed in the Circle (i.e. random initial state).\n",
    "\n",
    "![](https://raw.githubusercontent.com/yuzhenqin90/RLcourse/main/final/CircleWorldMDP.png)\n",
    "\n",
    "We will consider two variants of the task: an **episodic** version where 0 is a terminal state and a **continuing**\n",
    "version where there is no terminal state and the agent can just move indefinitely around the circle.\n",
    "\n",
    "The final assignment is built on this simple task--Circle World. It contains four parts, emcompassing all the content taught in the course:\n",
    " 1. Markov Decision Processes (*1 Points*)\n",
    " 2. Dynamic Programming (*4 Points*)\n",
    " 3. Monte Carlo methods (*2 Points*)\n",
    " 4. TD-learning (*3 Points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "#### Preliminaries\n",
    "\n",
    "Let's start by defining a generic environment base class.\n",
    "\n",
    "**NOTE:** For simplicity and to make sure you are always able to run the exercises, we provide a simple\n",
    "implementation in Python / numpy. You are encouraged to re-implement the MDP below using `gymnasium` (up to 2 bonus points,\n",
    " to a maximum final grade of 10, will be awarded if you do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CircleWorld():\n",
    "\t\"\"\"\n",
    "\tGeneric base class for MDPs with finite state, action and reward spaces\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_states, n_actions, reward, task, gamma=1.0, state_labels=None, action_labels=None):\n",
    "\t\t\"\"\"\n",
    "\t\tn_states: number of states [0,...,N-1]\n",
    "\t\tn_actions: number of actions [0,...,N-1]\n",
    "\t\treward: reward values\n",
    "\t\ttask: episodic or continuing\n",
    "\t\tgamma: discounting factor\n",
    "\t\t\"\"\"\n",
    "\t\tself.n_states = n_states\n",
    "\t\tself.n_actions = n_actions\n",
    "\t\tself.state_labels = state_labels or np.arange(self.n_states)\n",
    "\t\tself.action_labels = action_labels or np.arange(self.n_actions)\n",
    "\t\tself.n_rewards = len(reward)\n",
    "\t\tself.reward = reward\n",
    "\t\tassert(task == 'episodic' or task == 'continuing')\n",
    "\t\tself.task = task\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\t\"\"\"\n",
    "\t\tSample initial state at start of the episode; assumed uniform\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.random.randint(self.n_states)\n",
    "\n",
    "\tdef p_transition(self, s, a, s1, r):\n",
    "\t\t\"\"\"\n",
    "\t\tTransition density s x a => s1 x r\n",
    "\t\t\"\"\"\n",
    "\t\t# This only works for deterministic state transitions; otherwise override\n",
    "\t\treturn np.float64((s1, r) == self.step(s, a))\n",
    "\n",
    "\tdef step(self, s, a):\n",
    "\t\t\"\"\"\n",
    "\t\tSample new state and reward when starting in s and taking action a\n",
    "\t\t:return new_state, reward_index: note that there are 3\n",
    "\t\t\"\"\"\n",
    "\t\ts1 = (s + 2 * a - 1) % self.n_states  # takes one step left or right\n",
    "\t\tif s1 == 0:\n",
    "\t\t\tr = 0\n",
    "\t\telse:\n",
    "\t\t\tr = 1\n",
    "\t\treturn s1, r\n",
    "\n",
    "\tdef sample_action(self, state, policy):\n",
    "\t\t# sample action from policy for a given state\n",
    "\t\treturn np.random.choice(np.arange(self.n_actions), p=policy[state])\n",
    "\n",
    "\tdef sample_episode(self, policy, T=None):\n",
    "\t\t\"\"\"\n",
    "\t\tSample a finite horizon sequence from an MDP using some policy\n",
    "\t\tIf the tasks is continuing then we sample exactly T steps\n",
    "\t\tIf the task is episodic then we sample exactly one episode or reset until we sample T steps\n",
    "\t\t\"\"\"\n",
    "\t\t# sequence element is state, action, reward\n",
    "\t\tseq = []\n",
    "\t\t# randomly sample initial state NOTE: For exploring starts we would need to sample both states and actions. This is not needed for epsilon-greedy policies\n",
    "\t\ts = self.reset()\n",
    "\n",
    "\t\tif self.task == 'continuing':\n",
    "\t\t\tassert (T is not None)\n",
    "\t\t\tfor t in range(T):\n",
    "\t\t\t\ta = self.sample_action(s, policy)\n",
    "\t\t\t\t(s1, r) = self.step(s, a)\n",
    "\t\t\t\tseq.append([s, a, self.reward[r]])\n",
    "\t\t\t\ts = s1\n",
    "\t\telse:\n",
    "\t\t\tt = 0\n",
    "\t\t\twhile True:\n",
    "\n",
    "\t\t\t\tif T is None and self.is_terminal(s):\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\telif t == T:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\ta = self.sample_action(s, policy)\n",
    "\t\t\t\tif self.is_terminal(s):\n",
    "\t\t\t\t\ts1 = self.reset()\n",
    "\t\t\t\t\tr = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t(s1, r) = self.step(s, a)\n",
    "\t\t\t\tseq.append([s, a, self.reward[r]])\n",
    "\t\t\t\ts = s1\n",
    "\t\t\t\tt = t+1\n",
    "\t\treturn seq\n",
    "\n",
    "\tdef optimal_policy(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFor this simple task, the optimal policy is just to move to state zero as quickly as possible\n",
    "\t\tand then flip back and forth\n",
    "\t\t\"\"\"\n",
    "\t\tpolicy = np.zeros([self.n_states, self.n_actions])\n",
    "\t\tn = int(np.round(self.n_states/2))\n",
    "\t\tpolicy[:n, 0] = 1.0\n",
    "\t\tpolicy[n:, 1] = 1.0\n",
    "\t\tpolicy[self.terminal_states(), :] = 1.0 / self.n_actions\n",
    "\t\treturn policy\n",
    "\n",
    "\tdef random_deterministic_policy(self):\n",
    "\t\t\"\"\"\n",
    "\t\tRandom choice of a deterministic action for each state\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.random.multinomial(1, [1.0 / self.n_actions for a in range(self.n_actions)], self.n_states).astype('float32')\n",
    "\n",
    "\tdef nonterminal_states(self):\n",
    "\t\t\"\"\"\n",
    "\t\t:return: The set S of nonterminal states\n",
    "\t\t\"\"\"\n",
    "\t\treturn [s for s in range(self.n_states) if not self.is_terminal(s)]\n",
    "\n",
    "\tdef terminal_states(self):\n",
    "\t\t\"\"\"\n",
    "\t\t:return: The set S of terminal states\n",
    "\t\t\"\"\"\n",
    "\t\treturn [s for s in range(self.n_states) if self.is_terminal(s)]\n",
    "\n",
    "\tdef is_terminal(self, s):\n",
    "\t\t\"\"\"\n",
    "\t\tflags if s is a terminal state\n",
    "\t\t\"\"\"\n",
    "\t\tif self.task == \"episodic\":\n",
    "\t\t\treturn s == 0\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tdef q_to_v(self, q, policy):\n",
    "\t\t\"\"\"\n",
    "\t\tConvert Q-value to state value\n",
    "\t\t:param q:\n",
    "\t\t:param policy:\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tv = np.zeros(self.n_states)\n",
    "\t\tfor s in self.nonterminal_states():\n",
    "\t\t\tfor a in range(self.n_actions):\n",
    "\t\t\t\tv[s] += policy[s, a] * q[s, a]\n",
    "\t\treturn v\n",
    "\n",
    "\tdef v_to_q(self, v):\n",
    "\t\t\"\"\"\n",
    "\t\tConvert state value to Q-value\n",
    "\t\t:param v:\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tq = np.zeros([self.n_states, self.n_actions])\n",
    "\t\tfor s in self.nonterminal_states():\n",
    "\t\t\tfor a in range(self.n_actions):\n",
    "\t\t\t\tq[s, a] = sum(self.p_transition(s, a, s1, r) * (self.reward[r] + self.gamma * v[s1])\n",
    "\t\t\t\t              for s1 in range(self.n_states) for r in range(self.n_rewards))\n",
    "\t\treturn q\n",
    "\n",
    "\t# some simple rendering methods\n",
    "\tdef __str__(self):\n",
    "\t\tstr = \"task: {0}\\n\".format(self.task)\n",
    "\t\tstr += \"states: {0}\\n\".format(self.state_labels)\n",
    "\t\tif self.terminal_states():\n",
    "\t\t\tstr += \"terminal states: {0}\\n\".format(self.terminal_states())\n",
    "\t\tstr += \"actions: {0}\\n\".format(self.action_labels)\n",
    "\t\tstr += \"rewards: {0}\\n\".format(self.reward)\n",
    "\t\tstr += \"discounting factor: {0}\".format(self.gamma)\n",
    "\t\treturn str\n",
    "\n",
    "\tdef print_policy(self, policy):\n",
    "\t\tfor s in range(self.n_states):\n",
    "\t\t\ta = np.random.choice(np.arange(self.n_actions), p=policy[s])\n",
    "\t\t\tprint('state ' + str(self.state_labels[s]) + ' => action ' + str(self.action_labels[a]))\n",
    "\n",
    "\tdef print_value(self, vf):\n",
    "\t\t\"\"\"\n",
    "\t\t:param vf: state value or action value function\n",
    "\t\t\"\"\"\n",
    "\t\tif vf.ndim == 1:\n",
    "\t\t\tfor s in range(self.n_states):\n",
    "\t\t\t\tprint('state ' + str(self.state_labels[s]) + ': ' + str(vf[s]))\n",
    "\t\telse:\n",
    "\t\t\tfor s in range(self.n_states):\n",
    "\t\t\t\tfor a in range(self.n_actions):\n",
    "\t\t\t\t\tprint('state ' + str(self.state_labels[s]) + ' - action ' + str(self.action_labels[a] + ': ' + str(vf[s,a])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To evaluate longer episodes, we will primarily use the continuing version and specify a maximum timespan $T$ where we\n",
    "manually stop the simulation. Depending on the class of algorithms and their specificities, we will alternate between\n",
    " the continuing and episodic versions of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: continuing\n",
      "states: [0 1 2 3 4 5 6 7 8 9]\n",
      "actions: ['L', 'R']\n",
      "rewards: [1.0, -0.1111111111111111]\n",
      "discounting factor: 0.99\n",
      "task: episodic\n",
      "states: [0 1 2 3 4 5 6 7 8 9]\n",
      "terminal states: [0]\n",
      "actions: ['L', 'R']\n",
      "rewards: [1.0, -0.1111111111111111]\n",
      "discounting factor: 1.0\n"
     ]
    }
   ],
   "source": [
    "mdpc = CircleWorld(n_states=10, n_actions=2, reward=[1.0, -1.0 /(10 - 1)], task='continuing', gamma=0.99,\n",
    "                   state_labels=None, action_labels=['L', 'R'])\n",
    "print(mdpc)\n",
    "\n",
    "mdpe = CircleWorld(n_states=10, n_actions=2, reward=[1.0, -1.0 /(10 - 1)], task='episodic', gamma=1.,\n",
    "                   state_labels=None, action_labels=['L', 'R'])\n",
    "print(mdpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given the simplicity of the task, the optimal policy simply requires moving to state 0 as quickly as possible, so we\n",
    "can construct an explicit optimal policy which will be used as baseline for comparisons and performance evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0 => action L\n",
      "state 1 => action L\n",
      "state 2 => action L\n",
      "state 3 => action L\n",
      "state 4 => action L\n",
      "state 5 => action R\n",
      "state 6 => action R\n",
      "state 7 => action R\n",
      "state 8 => action R\n",
      "state 9 => action R\n"
     ]
    }
   ],
   "source": [
    "# to understand the policy, study the `optimal_policy` method defined above\n",
    "pi_optimal_c = mdpc.optimal_policy()\n",
    "mdpc.print_policy(pi_optimal_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 0 => action R\n",
      "state 1 => action L\n",
      "state 2 => action L\n",
      "state 3 => action L\n",
      "state 4 => action L\n",
      "state 5 => action R\n",
      "state 6 => action R\n",
      "state 7 => action R\n",
      "state 8 => action R\n",
      "state 9 => action R\n"
     ]
    }
   ],
   "source": [
    "pi_optimal_e = mdpe.optimal_policy()\n",
    "pd.DataFrame({\"p(L|s)\": pi_optimal_e[:,0], \"p(R|s)\": pi_optimal_e[:,1]})\n",
    "mdpe.print_policy(pi_optimal_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Having this ground truth ($\\pi_{*}$, $v_{*}$ and $q_{*}$) we can evaluate how accurate are the corresponding estimates\n",
    " from the different algorithms ($\\hat{\\pi}$, $V(s)$ and $Q(s,a)$) by determining the mean-squared error between\n",
    " the. For example, having an estimate of $V$, the distance between this estimate and the true value function is given\n",
    "  by:\n",
    "\n",
    "$$MSE(V, v_{\\pi}) = \\sum_{s} (V(s) - v_{\\pi}(s))^{2}$$\n",
    "\n",
    "This way of measuring the accuracy of the estimates will be used throughout the assignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Markov Decision Processes *(1 point)*\n",
    "\n",
    "**a**) Define a uniform / equiprobable stochastic policy (all actions have equal probability in all states). Note\n",
    "that a policy in the current implementation should be defined as an [`n_states` x `n_actions`] array. ---------(**0.2 point**)\n",
    "\n",
    "The cell below\n",
    "illustrates an example deterministic policy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p(L|s)</th>\n",
       "      <th>p(R|s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p(L|s)  p(R|s)\n",
       "0     0.0     1.0\n",
       "1     1.0     0.0\n",
       "2     1.0     0.0\n",
       "3     0.0     1.0\n",
       "4     1.0     0.0\n",
       "5     0.0     1.0\n",
       "6     0.0     1.0\n",
       "7     0.0     1.0\n",
       "8     1.0     0.0\n",
       "9     0.0     1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_policy = mdpc.random_deterministic_policy()\n",
    "pol = pd.DataFrame({\"p(L|s)\": example_policy[:,0], \"p(R|s)\": example_policy[:,1]})\n",
    "pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def uniform_stochastic_policy(n_states, n_actions):\n",
    "\t\"\"\"\n",
    "\tEach action has equal probability in all states\n",
    "\t\"\"\"\n",
    "\t# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**b**) Sample an \"episode\" from the MDP (use `mdpc`, but cut-off the simulation at 500 timesteps), under the uniform\n",
    "stochastic policy you defined above and plot the evolution of the returns\n",
    "$G_{t}$ obtained under this policy, with discounting\n",
    "factors $\\gamma=1$, $\\gamma=0.99$ and $\\gamma=0.8$. Note that the sample episode is a sequence of `[state, action,\n",
    "reward]`, so the returns have to be calculated from the rewards obtained. ---------(**0.2 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**c**) What is the problem of using $\\gamma=1$ for this task? How was this problem solved in **b**? ---------(**0.2 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**d**) Sample an episode from the episodic version of the task (implemented above as `mdpe`), under the uniform\n",
    "stochastic policy\n",
    "and plot the evolution of the returns\n",
    "$G_{t}$ obtained under this policy, with discounting\n",
    "factors $\\gamma=1$, $\\gamma=0.99$ and $\\gamma=0.8$. ---------(**0.2 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**e**) Rerun the continuing task with a fixed $T=500$, using the pre-implemented deterministic policy and plot the\n",
    "evolution of the returns $G_{t}$ obtained under this policy, with discounting\n",
    "factors $\\gamma=1$, $\\gamma=0.99$ and $\\gamma=0.8$.\n",
    "---------(**0.2 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Dynamic Programming *(4 points)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**a**) **Policy evaluation**: DP-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**a.1**) Write a policy evaluation function, which computes the state-value function from a given policy, $v_{\\pi}$. ---------(**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, policy, theta=1e-8, V=None, max_t=None):\n",
    "\t\"\"\"\n",
    "\tIterative policy evaluation\n",
    "\t:param mdp: the mdp object\n",
    "\t:param policy: the policy to evaluate\n",
    "\t:param value: the value function (can be initialized)\n",
    "\t:param theta: cutoff for policy evaluation\n",
    "\t:param max_t: maximum number of steps\n",
    "\tReturns: State value function V\n",
    "\t\"\"\"\n",
    "\t# Conditions for convergence\n",
    "\tassert (mdp.task == 'episodic' or mdp.gamma < 1.0)\n",
    "\t# Init V\n",
    "\tif V is None:\n",
    "\t\tV = np.zeros(mdp.n_states)\n",
    "\n",
    "\t# Evaluate policy iteratively\n",
    "\tt = 0\n",
    "\twhile True:\n",
    "\t\t# TODO\n",
    "\n",
    "\treturn V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**a.2**) Evaluate the optimal policy generated above, using the `policy_evaluation` function you just implemented.\n",
    "Compare the value function ($v_{\\pi}$) for the optimal policy with that obtained for the deterministic random policy\n",
    "(pre-implemented) and for the stochastic, uniform random policy you implemented in 1a). Print or plot the results. ---------(**0.4 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**a.3**) Evaluate the optimal policies for the continuing and the episodic tasks and plot the resulting value\n",
    "functions $v_{*}$. What is the main difference in state values? Why is this the case? ---------(**0.4 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**b**) **Policy iteration** versus **value iteration**: DP-control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The control problem is solved by following a sequence of policy *evaluation* and *improvement*:\n",
    "\n",
    "$$\\pi_{0} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{0}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{1}\n",
    "\\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{1}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{2}\n",
    "\\stackrel{\\mathrm{E}}{\\longrightarrow} \\cdots \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{*}\n",
    "\\stackrel{\\mathrm{E}}{\\longrightarrow} v_{*}$$\n",
    "\n",
    "In the exercise above, we have implemented the `policy_evaluation` step, which, given a policy $\\pi$, returns the\n",
    "corresponding state value function $v_{\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "**b.1**) Implement a `policy_improvement` function\n",
    "where, given a policy and a state value function, computes action-values $q_{\\pi}$ and uses those to improve the\n",
    "policy by selecting the actions with the highest $q_{\\pi}$ for every given state, i.e. acting greedy with respect to\n",
    "the Q-values to improve the policy. ---------(**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(mdp, policy, V):\n",
    "\t\"\"\"\n",
    "\tPolicy improvement operates directly on the input policy.\n",
    "\t:param mdp:\n",
    "\t:param policy:\n",
    "\t:param V: state-value function\n",
    "\treturns: policy, policy_stable (bool)\n",
    "\t\"\"\"\n",
    "\t# TODO\n",
    "\treturn policy, policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**b.2**) Implement the complete `policy_iteration` algorithm by interleaving\n",
    "steps of *evaluation* and *improvement*, using the functions implemented above. Note: this should be implemented as a\n",
    "function that takes in an initial policy $\\pi_{0}$ and returns a final (optimal) policy and a\n",
    "sequence (list) of value functions. Run your *policy iteration* algorithm starting with the stochastic uniform policy,\n",
    "print the resulting policy and plot the evolution of the evaluated value functions. ---------(**0.6 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy):\n",
    "\t\"\"\"\n",
    "\treturns policy and list of value estimates per state and wall-clock time\n",
    "\toperates directly on the input policy\n",
    "\t\"\"\"\n",
    "\tstart = time.time()\n",
    "\tV = None\n",
    "\tVs = []\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\tend = time.time()\n",
    "\tprint(\"{0} iterations completed in {1} ms\".format(len(Vs), end-start))\n",
    "\n",
    "\treturn policy, Vs, end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "optimized_policy_pi, value_functions_pi, timings_pi = policy_iteration(mdpc, uniform_stochastic_policy(mdpc.n_states,\n",
    "                                                                                                       mdpc.n_actions))\n",
    "mdpc.print_policy(optimized_policy_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**b.3**) Implement the `value_iteration` algorithm, where you skip the policy improvement steps until you reach the\n",
    "optimal value function, from which a final policy can be computed.\n",
    "\n",
    "$$\\pi_{0} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{0} {\\longrightarrow} v_{1} {\\longrightarrow} v_{2}\n",
    "{\\longrightarrow} \\cdots {\\longrightarrow} v_{*} \\rightarrow \\pi_{*}$$\n",
    "\n",
    "Note: this should be implemented as a function which starts with an all-zeros value function $v_{0}=0 \\\n",
    "\\forall s \\in \\mathcal{S}$ and returns a final (optimal) policy and a sequence of value functions. To construct the\n",
    "final policy, you will also need a function to construct a policy based on a state value function (this is provided below for simplicity).\n",
    "Run your *value iteration* algorithm starting with the stochastic uniform policy, print\n",
    " the resulting policy and plot the evolution of the value functions. ---------(**0.6 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def policy_construction(mdp, V):\n",
    "\t\"\"\"\n",
    "\tDerive policy from value function\n",
    "\t\"\"\"\n",
    "\tpolicy = np.ones([mdp.n_states, mdp.n_actions]) / mdp.n_actions\n",
    "\tfor s in range(mdp.n_states):\n",
    "\t\taction_values = np.zeros(mdp.n_actions)\n",
    "\t\tfor a in range(mdp.n_actions):\n",
    "\t\t\taction_values[a] = sum(mdp.p_transition(s, a, s1, r) * (mdp.reward[r] + mdp.gamma * V[s1])\n",
    "\t\t\t                       for s1 in range(mdp.n_states) for r in range(mdp.n_rewards))\n",
    "\t\tbest_action = np.argmax(action_values)\n",
    "\t\t# Update Policy\n",
    "\t\tpolicy[s, :] = 0\n",
    "\t\tpolicy[s, best_action] = 1\n",
    "\treturn policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, theta=1e-8, max_t=None):\n",
    "\t\"\"\"\n",
    "\treturns policy and list of value estimates per state\n",
    "\t\"\"\"\n",
    "\tstart = time.time()\n",
    "\tV = np.zeros(mdp.n_states)\n",
    "\n",
    "\t# Evaluate policy iteratively\n",
    "\tt = 0\n",
    "\tVs = []\n",
    "\twhile True:\n",
    "\t\t# TODO\n",
    "\n",
    "\t# Output deterministic policy\n",
    "\tpolicy = policy_construction(mdp, V)\n",
    "\tend = time.time()\n",
    "\tprint(\"{0} iterations completed in {1} ms\".format(len(Vs), end-start))\n",
    "\treturn policy, Vs, end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "optimized_policy_vi, value_functions_vi, timings_vi = value_iteration(mdpc)\n",
    "mdpc.print_policy(optimized_policy_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given that we know the ground truth for this simple problem, we can evaluate the performance of the\n",
    "different algorithms by measuring how far they are from the optimal solution. So, we can calculate the Mean Squared\n",
    "Error (MSE) and plot it as a function of time to see how fast and accurate are the different algorithms. **Note:** In\n",
    " this initial example, we will provide a base implementation, but this procedure will be repeated to\n",
    " evaluate the different classes of solutions (here, the MSE is plotted as a function of computing time, but in the\n",
    " model-free algorithms it will be plotted as a function of the number of episodes used).\n",
    "\n",
    "  ***Notice:*** This is not a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "time_axis_pi = np.arange(0, timings_pi, timings_pi / (len(value_functions_pi)+1))[:len(value_functions_pi)]\n",
    "ax[0].plot(time_axis_pi, list(map(lambda x: np.mean((x - V_c)**2), value_functions_pi)), '-o', label='policy iteration')\n",
    "time_axis_vi = np.arange(0, timings_vi, timings_vi / (len(value_functions_vi)+1))[:len(value_functions_vi)]\n",
    "ax[0].plot(time_axis_vi, list(map(lambda x: np.mean((x - V_c)**2), value_functions_vi)), '-x', label='value iteration')\n",
    "ax[0].set_xlabel('Time [ms]')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[0].set_title('State value')\n",
    "\n",
    "ax[1].plot(time_axis_pi, list(map(lambda x: np.mean((mdpc.v_to_q(x) - mdpc.v_to_q(V_c))**2), value_functions_pi)),\n",
    "           '-o', label='policy iteration')\n",
    "ax[1].plot(time_axis_vi, list(map(lambda x: np.mean((mdpc.v_to_q(x) - mdpc.v_to_q(V_c))**2), value_functions_vi)),\n",
    "           '-o', label='value iteration')\n",
    "ax[1].set_xlabel('Time [ms]')\n",
    "ax[1].set_ylabel('MSE')\n",
    "ax[1].set_title('Q-value')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Policy MSE: {} (policy iteration)\".format(np.mean(np.array(pi_optimal_e) - np.array(optimized_policy_pi))**2))\n",
    "print(\"Policy MSE: {} (value iteration)\".format(np.mean(np.array(pi_optimal_e) - np.array(optimized_policy_vi))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Monte Carlo Methods *(2 points)*\n",
    "\n",
    "For this exercise, we will use the *episodic* version of the MDP (`mdpe` instantiated above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**a**) Why do we need to use an *episodic* task? ---------(**0.2 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**b**) **MC Prediction**: Implement an *on-policy, first-visit MC* algorithm to estimate the state-value function for\n",
    "the optimal policy\n",
    ". This should be provided as a function `mc_prediction` that takes in the initial policy and\n",
    " a parameter specifying how many sample episodes to run. Use your implementation to calculate the approximate state\n",
    " value function $V_{\\pi}$ for the optimal policy $\\pi_{*}$. Plot the MSE between the estimated value function and the\n",
    "  true optimal $v_{*}$ as a function of number of episodes, i.e. how the accuracy of the estimate evolves across\n",
    "  episodes. ---------(**0.9 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def mc_prediction(mdp, policy, num_simulations=30):\n",
    "\t# Conditions for convergence\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\tV = np.zeros(mdp.n_states)\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\treturn V, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "V, Vs = mc_prediction(mdpe, mdpe.optimal_policy(), num_simulations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**c**) **MC Control**: Implement an *on-policy, first-visit MC* algorithm to optimize the policy. To do this, you will\n",
    "need to\n",
    "estimate the Q-value function (computing returns after first visit to state-action pairs) and use it to take an\n",
    "$\\epsilon$-greedy action selection to gradually improve the policy. This should be provided as a function\n",
    "`mc_control` that takes in the initial policy, the number of episodes to use and `epsilon` to set the mininum action\n",
    "probability. Run for 200 episodes and plot the MSE between the estimated q-value function and the\n",
    "  true optimal $q_{*}$ as a function of number of episodes. Note: you can obtain the $q_{*}$ from\n",
    "$v_{*}$ using the method `mdpe.v_to_q()`. ---------(**0.9 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def mc_control(mdp, policy, num_simulations=30, epsilon=0.01):\n",
    "\t\"\"\"\n",
    "\tRun the Monte Carlo First Visit On-Policy algorithm and return the estimated\n",
    "\tpolicy, Q (state action) values, and returns (rewards) dict.\n",
    "\tUses epsilon-soft policies instead of exploring starts\n",
    "\t:param mdp:\n",
    "\t:param policy: any epsilon soft policy; e.g. mdp.uniform_stochastic_policy()\n",
    "\t:param num_simulations : int Number of episodes for the policy iteration process\n",
    "\t:param epsilon: epsilon-soft minimum probability\n",
    "\n",
    "\t:return policy: numpy.ndarray Estimated Policy\n",
    "\t:return Q: numpy.ndarray Estimated Q (state-action) values\n",
    "\t:return R: dict Returns obtained for every state\n",
    "\t\"\"\"\n",
    "\t# Conditions for convergence\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\n",
    "\tQ = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\treturn policy, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "policy0 = uniform_stochastic_policy(mdpe.n_states, mdpe.n_actions)\n",
    "policy_mc, Qs_mc = mc_control(mdpe, policy0, num_simulations=10000, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy MSE: {}\".format(np.mean(np.array(pi_e) - np.array(policy_mc))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "### 4. TD-methods *(3 points)*\n",
    "\n",
    "Note that, despite the fact that TD-learning can handle continuing tasks, we will constrain the study to episodic\n",
    "tasks because the nature of the environment would result in infinite loops where no stable solutions can be found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**a**) **TD Prediction**: Implement and run TD(0) to estimate the value function under the optimal policy. Run the\n",
    "algorithm for 500 sample episodes with a fixed $\\alpha=0.01$. Plot the result as the MSE between the state\n",
    " value estimates $V$ and the ground truth $v_{\\pi}$ as a function of number of episodes, i.e. how\n",
    " the accuracy of the estimate evolves across\n",
    "  episodes. ---------(**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def td0_prediction(mdp, policy, num_simulations=30, alpha = 0.01):\n",
    "\t\"\"\"\n",
    "\tTD(0) prediction algorithm\n",
    "\t:param mdp:\n",
    "\t:param policy:\n",
    "\t:param num_simulations: number of episodes to sample\n",
    "\t:param alpha: fixed learning rate\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\t# Conditions for convergence; note that we can also run on continuing problems\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\tV = np.zeros(mdp.n_states)\n",
    "\tVs = []\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\treturn V, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "td_v, td_vs = td0_prediction(mdpe, mdpe.optimal_policy(), num_simulations=500, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**b**) **On-policy TD control**: Implement and run the SARSA algorithm to estimate the optimal policy. Remember\n",
    "that policy updates are $\\epsilon$-greedy. Start from a uniform stochastic policy and run the algorithm for 10000\n",
    "episodes, with a fixed learning rate $\\alpha=0.01$ and $\\epsilon=0.1$. Plot the MSE between the estimated q-value\n",
    "function and the true optimal $q_{*}$ as a function of number of episodes. Note: you can obtain the $q_{*}$ from\n",
    "$v_{*}$ using the method `mdpe.v_to_q()`. ---------(**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def sarsa(mdp, num_simulations=30, alpha=0.01, epsilon=0.1):\n",
    "\t\"\"\"\n",
    "\tSARSA on-policy control\n",
    "\t:param mdp:\n",
    "\t:param num_simulations: number of sample episodes\n",
    "\t:param alpha: learning rate\n",
    "\t:param epsilon: minimum action selection probability\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\t# Conditions for convergence; note that we can also run on continuing problems\n",
    "\tassert (mdp.task == 'episodic')\n",
    "\tQ = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\tpolicy = uniform_stochastic_policy(mdp.n_states, mdp.n_actions)\n",
    "\tQs = []\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\treturn policy, Q, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "policy_sarsa, Q, Qs = sarsa(mdpe, num_simulations=10000, alpha=0.01, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**c**) **Off-policy TD control**: Implement and run the Q-learning algorithm to estimate the optimal policy. Start from a uniform stochastic policy and run the algorithm for 10000\n",
    "episodes, with a fixed learning rate $\\alpha=0.01$ and $\\epsilon=0.1$. Plot the MSE between the estimated q-value\n",
    "function and the true optimal $q_{*}$ as a function of number of episodes. ---------(**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def qlearning(mdp, behavioral_policy, num_simulations=30, alpha=0.01, epsilon=0.1):\n",
    "\t# Conditions for convergence; note that we can also run on continuing problems\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\n",
    "\tQ = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\tQs = []\n",
    "\n",
    "\tfor t in range(num_simulations):\n",
    "\t\ts = mdp.reset()\n",
    "\t\twhile not mdp.is_terminal(s):\n",
    "\t\t\ta = mdp.sample_action(s, behavioral_policy)\n",
    "\t\t\t(s1, r) = mdp.step(s, a)\n",
    "\n",
    "\t\t\t# TODO: complete\n",
    "\n",
    "\t\tQs.append(copy.copy(Q))\n",
    "\t# determine policy from Q function\n",
    "\ttarget_policy = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\tfor state in mdp.terminal_states():\n",
    "\t\ttarget_policy[state,:] = 1.0 / mdp.n_actions\n",
    "\tfor state in mdp.nonterminal_states():\n",
    "\t\ta_max = np.random.choice(np.flatnonzero(Q[state] == np.max(Q[state])))\n",
    "\t\ttarget_policy[state, a_max] = 1.0\n",
    "\n",
    "\treturn target_policy, Q, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "b_policy = uniform_stochastic_policy(mdpe.n_states, mdpe.n_actions)\n",
    "policy_Q, Q, Qs = qlearning(mdpe, behavioral_policy=b_policy, num_simulations=10000, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. Comparing tabular methods *(2 bonus points)*\n",
    "\n",
    "Now that you have all algorithms implemented, change the task to include a finer grid on the circle world. Create a\n",
    "CircleWorld environment with a larger number of states and re-run the control algorithms. Plot the results side-by-side\n",
    " to directly\n",
    "compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
