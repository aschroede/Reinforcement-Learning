{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# **Reinforcement Learning** [SOW-BKI258]\n",
    "## Practical 1: Introduction (demos)\n",
    "\n",
    "In today's practical, we will have some fun with modern RL tools and software. We will explore some demos and\n",
    "examples that illustrate the usefulness, interest (and fun) of RL algorithms and how easy it is to use them with modern software tools.\n",
    "\n",
    "**NOTE:** This week's practical is merely a demonstration/tutorial. There is no assignment to hand-in. In part 2, we\n",
    "will superficially explore some advanced topics in modern RL and see a concrete demo, but most of the content of this\n",
    " demo (models and environments) are beyond the scope of this course. Thus, you are not asked to understand the models used and you will not be evaluated about this content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1. `OpenAI-gym`\n",
    "\n",
    "We will start by going through the basics of [OpenAI Gym](https://www.gymlibrary.dev/index.html), the most popular choice to define environments and agent-environment interactions. It comes with a set of handy tools as well as pre-defined (classical) RL environments / tasks. The use of `gym` will come up several times during the practical exercises, so try to get an early grasp of the basic concepts involved (these will, of course, be explained in the coming weeks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 1.1. Let's look at a couple of example of a classic RL task: the `LunarLander` and the `CartPole` environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## make sure the version of installed Python (in your environment) is one of: Python 3.8, 3.9, 3.10, 3.11\n",
    "\n",
    "##You may need to install some packages initially, \n",
    "## for Mac users, un-comment the following lines to complete the installation\n",
    "\n",
    "#pip install gymnasium \n",
    "#pip install swig\n",
    "#pip install 'gymnasium[box2d]'\n",
    "\n",
    "\n",
    "#For Windows users, copy and run the following commands in the Anaconda terminal window\n",
    "# pip install gymnasium \n",
    "# pip install swig\n",
    "# pip install 'gymnasium[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load the gym environment and set the render mode to \"human\" for us to visualize the outputs\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# seed the action space and environment (so the results can be reproduced)\n",
    "env.action_space.seed(42)\n",
    "observation, info = env.reset(seed=42)\n",
    "# randomly sample the action space (take random actions) to see how the environment looks like\n",
    "for _ in range(1000):\n",
    "\tobservation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\tif terminated or truncated:\n",
    "\t\tobservation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "\tenv.render()\n",
    "\tobservation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\tif terminated:\n",
    "\t\tobservation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      5\u001b[0m \tenv\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m----> 6\u001b[0m \tobservation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m terminated:\n\u001b[1;32m      8\u001b[0m \t\tobservation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Documents/Reinforcement Learning/RL/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/Reinforcement Learning/RL/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Reinforcement Learning/RL/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Reinforcement Learning/RL/lib/python3.10/site-packages/gymnasium/envs/classic_control/acrobot.py:227\u001b[0m, in \u001b[0;36mAcrobotEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    224\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ob(), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {})\n",
      "File \u001b[0;32m~/Documents/Reinforcement Learning/RL/lib/python3.10/site-packages/gymnasium/envs/classic_control/acrobot.py:367\u001b[0m, in \u001b[0;36mAcrobotEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v3', render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "\tenv.render()\n",
    "\tobservation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\tif terminated:\n",
    "\t\tobservation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can check the attributes and methods of the environments to understand what it is made of. I advise you to check\n",
    "the official documentation and, for example, [this tutorial](https://www.gymlibrary.dev/content/basic_usage/) to gain a better insight into `gym` environments.\n",
    "\n",
    "Particularly important are the state and action spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "The action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(\"The observation space: {}\".format(obs_space))\n",
    "print(\"The action space: {}\".format(action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the cartpole environment, the states are the (discretized / binned) position, velocity, pole angle and pole\n",
    "angular velocity. There are only 2 possible actions: `move left` and `move right`. For more info on the task, see\n",
    "[documentation](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.2. Interacting with the environment\n",
    "\n",
    "Preconfigured `gym` environments are typically accompanied by sophisticated visualizations and rendering tools that\n",
    "allow us to see how the agent is learning the task at hand. More importantly, `gym` provides a wrapper for any new RL environment, exposing the following main API methods:\n",
    " *  `step`: Receives an action (`int`) and returns 4 objects: the next state (observation), the reward, a boolean\n",
    " indicating whether the environment has ended, and any extra info depending on the environment.\n",
    "*  `reset`: Reset the environment, returns the initial state / observation.\n",
    "*  `render`: Returns an object for rendering, depending on the `mode` parameter passed in.\n",
    "\n",
    "See [here](https://github.com/openai/gym/blob/3bd5ef71c2ca3766a26c3dacf87a33a9390ce1e6/gym/core.py) for more details on this API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we have done in the demo above, we use these methods to interact with pre-existing environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial observation is (array([-0.4689014,  0.       ], dtype=float32), {})\n",
      "The new observation is [-0.4703098  -0.00140839]\n"
     ]
    }
   ],
   "source": [
    "# import env\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "\n",
    "# reset the environment and see the initial observation\n",
    "observation = env.reset()\n",
    "print(\"The initial observation is {}\".format(observation))\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "new_observation, reward, terminated, truncated, info = env.step(random_action)\n",
    "print(\"The new observation is {}\".format(new_observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To run the RL environment, we loop through a number of steps, as we did in the demos above. Note that the goal of RL\n",
    "algorithms is to select actions that maximize rewards. In these examples, we are using a random *policy*, i.e.\n",
    "actions are randomly sampled from the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Number of steps you run the agent for\n",
    "num_steps = 150\n",
    "obs = env.reset()\n",
    "for step in range(num_steps):\n",
    "\t# take random action, but you can also do something more intelligent\n",
    "\t# action = my_intelligent_agent_fn(obs)\n",
    "\taction = env.action_space.sample()\n",
    "\n",
    "\t# apply the action\n",
    "\tobs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "\t# Render the env\n",
    "\tenv_img = env.render() # in this case, return a 2D rgb-array\n",
    "\n",
    "\t# If the epsiode is up, then start another one\n",
    "\tif terminated or truncated:\n",
    "\t\tenv.reset()\n",
    "\n",
    "# Close the env\n",
    "env.close()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(env_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 1.2. Now let's see how we can create a new `env` object from scratch\n",
    "\n",
    "Suppose you want to program a controller that can regulate the heat of your shower and get it in an optimal range.\n",
    "Some constraints on the problem:\n",
    "- we want our optimal temperature to be between 37 and 39 degrees Celsius.\n",
    "- The shower length will be 60 seconds (episode length = 60 seconds).\n",
    "- Three actions can be performed: `turn up`, `leave`, and `turn down`.\n",
    "\n",
    "(for a more completed example, see [this tutorial](https://medium.com/geekculture/developing-reinforcement-learning-environment-using-openai-gym-f510b0393eb7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "\tdef __init__(self):\n",
    "\t\tself.action_space = Discrete(3)\n",
    "\t\tself.observation_space = Box(low=np.array([0], dtype=np.float32), high=np.array([100], dtype=np.float32))\n",
    "\t\tself.state = 38 + random.randint(-3,3)\n",
    "\t\tself.shower_duration = 60\n",
    "\n",
    "\tdef step(self, action):\n",
    "\t\tself.state += action -1\n",
    "\t\tself.shower_duration -= 1\n",
    "\n",
    "\t\t# Calculating the reward\n",
    "\t\tif 37 <= self.state <= 39:\n",
    "\t\t\treward = 1\n",
    "\t\telse:\n",
    "\t\t\treward = -1\n",
    "\n",
    "\t\t\t# Checking if shower is done\n",
    "\t\tif self.shower_duration <= 0:\n",
    "\t\t\tdone = True\n",
    "\t\telse:\n",
    "\t\t\tdone = False\n",
    "\n",
    "\t\t# Setting the placeholder for info\n",
    "\t\tinfo = {}\n",
    "\n",
    "\t\t# Returning the step information\n",
    "\t\treturn self.state, reward, done, info\n",
    "\n",
    "\tdef render(self):\n",
    "\t\t# This is where you would write the visualization code (we will leave it for now)\n",
    "\t\tpass\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.state = 38 + random.randint(-3,3)\n",
    "\t\tself.shower_duration = 60\n",
    "\t\treturn self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "env = CustomEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-48\n",
      "Episode:2 Score:6\n",
      "Episode:3 Score:-60\n",
      "Episode:4 Score:-16\n",
      "Episode:5 Score:-60\n",
      "Episode:6 Score:-46\n",
      "Episode:7 Score:12\n",
      "Episode:8 Score:-38\n",
      "Episode:9 Score:-60\n",
      "Episode:10 Score:-60\n",
      "Episode:11 Score:-16\n",
      "Episode:12 Score:-38\n",
      "Episode:13 Score:-50\n",
      "Episode:14 Score:-60\n",
      "Episode:15 Score:28\n",
      "Episode:16 Score:-60\n",
      "Episode:17 Score:-12\n",
      "Episode:18 Score:10\n",
      "Episode:19 Score:-54\n",
      "Episode:20 Score:-18\n"
     ]
    }
   ],
   "source": [
    "episodes = 20 #20 shower episodes\n",
    "for episode in range(1, episodes+1):\n",
    "\tstate = env.reset()\n",
    "\tdone = False\n",
    "\tscore = 0\n",
    "\n",
    "\twhile not done:\n",
    "\t\taction = env.action_space.sample()\n",
    "\t\tn_state, reward, done, info = env.step(action)\n",
    "\t\tscore+=reward\n",
    "\tprint('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "With these examples and the linked tutorials, you should get a good outlook of how to use `gym`. Naturally, what\n",
    "actually matters is what comes next: how to choose the actions to take at any given time point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. Mario DQN\n",
    "\n",
    "To complement the lecture content and to demonstrate how modern RL algorithms can do fun things, explore [this\n",
    "tutorial](https://ml-showcase.paperspace.com/projects/super-mario-bros-double-deep-q-network) to see a practical and\n",
    "complete example of an agent learning the Super Mario Bros.\n",
    "\n",
    "**Note:** This tutorial is just for fun. The actual RL algorithm used (Deep Q-Network) is beyond the scope of the\n",
    "current course. We will go through the basis, including Q-learning, but this is an example of a Deep-RL algorithm.\n",
    "So, explore and have fun with the implementation, but don't get frightened or confused by the actual agent /\n",
    "algorithm used. In time, you will learn more about these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
