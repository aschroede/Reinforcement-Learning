{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-assignment 4:  TD Learning *(3 points)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This Jupyter Notebook explores the implementation of TD learning. TD learning, short for Temporal Difference learning, is a reinforcement learning algorithm that combines elements of both Monte Carlo methods and dynamic programming.\n",
    "\n",
    "In this notebook, we will focus on three different TD methods: TD prediction, on-policy TD control, and off-policy TD control. We will implement and run these algorithms on a specific environment called CircleWorld.\n",
    "\n",
    "The CircleWorld environment is a grid-like world where an agent can move in four directions: up, down, left, and right. The goal of the agent is to navigate through the world and reach a specific target state. The environment is episodic, meaning that each episode starts from a specific initial state and ends when the agent reaches the target state.\n",
    "\n",
    "We will start by implementing TD prediction, which aims to estimate the value function under the optimal policy. We will run the TD(0) algorithm for a fixed number of sample episodes and observe how the accuracy of the value function estimate evolves across episodes.\n",
    "\n",
    "Next, we will move on to on-policy TD control, where we will implement and run the SARSA algorithm. SARSA is an on-policy control algorithm that estimates the optimal policy by updating the action-value function based on the agent's experience. We will start with a uniform stochastic policy and gradually improve it through the SARSA algorithm.\n",
    "\n",
    "Finally, we will explore off-policy TD control using the Q-learning algorithm. Q-learning is an off-policy control algorithm that estimates the optimal policy by updating the action-value function based on the maximum Q-value of the next state. We will start with a uniform stochastic policy and observe how the estimated Q-values evolve across episodes.\n",
    "\n",
    "Throughout this notebook, we will plot the results of each algorithm to visualize the learning progress and compare the performance of different TD methods.\n",
    "\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-assignment is built on top of the previous sub-assignments, so it is important to complete them before, and utilize defined components in this sub-assignment. \n",
    "\n",
    "Note that, despite the fact that TD-learning can handle continuing tasks, we will constrain the study to episodic\n",
    "tasks because the nature of the environment would result in infinite loops where no stable solutions can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**a**) **TD Prediction**: Implement and run TD(0) to estimate the value function under the optimal policy. Run the\n",
    "algorithm for 500 sample episodes with a fixed $\\alpha=0.01$. Plot the result as the MSE between the state\n",
    " value estimates $V$ and the ground truth $v_{\\pi}$ as a function of number of episodes, i.e. how\n",
    " the accuracy of the estimate evolves across\n",
    "  episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def td0_prediction(mdp, policy, num_simulations=30, alpha = 0.01):\n",
    "\t\"\"\"\n",
    "\tTD(0) prediction algorithm\n",
    "\t:param mdp:\n",
    "\t:param policy:\n",
    "\t:param num_simulations: number of episodes to sample\n",
    "\t:param alpha: fixed learning rate\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\t# Conditions for convergence; note that we can also run on continuing problems\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\tV = np.zeros(mdp.n_states)\n",
    "\tVs = []\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\treturn V, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "td_v, td_vs = td0_prediction(mdpe, mdpe.optimal_policy(), num_simulations=500, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**b**) **On-policy TD control**: Implement and run the SARSA algorithm to estimate the optimal policy. Remember\n",
    "that policy updates are $\\epsilon$-greedy. Start from a uniform stochastic policy and run the algorithm for 10000\n",
    "episodes, with a fixed learning rate $\\alpha=0.01$ and $\\epsilon=0.1$. Plot the MSE between the estimated q-value\n",
    "function and the true optimal $q_{*}$ as a function of number of episodes. Note: you can obtain the $q_{*}$ from\n",
    "$v_{*}$ using the method `mdpe.v_to_q()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def sarsa(mdp, num_simulations=30, alpha=0.01, epsilon=0.1):\n",
    "\t\"\"\"\n",
    "\tSARSA on-policy control\n",
    "\t:param mdp:\n",
    "\t:param num_simulations: number of sample episodes\n",
    "\t:param alpha: learning rate\n",
    "\t:param epsilon: minimum action selection probability\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\t# Conditions for convergence; note that we can also run on continuing problems\n",
    "\tassert (mdp.task == 'episodic')\n",
    "\tQ = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\tpolicy = uniform_stochastic_policy(mdp.n_states, mdp.n_actions)\n",
    "\tQs = []\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\treturn policy, Q, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "policy_sarsa, Q, Qs = sarsa(mdpe, num_simulations=10000, alpha=0.01, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**c**) **Off-policy TD control**: Implement and run the Q-learning algorithm to estimate the optimal policy. Start from a uniform stochastic policy and run the algorithm for 10000\n",
    "episodes, with a fixed learning rate $\\alpha=0.01$ and $\\epsilon=0.1$. Plot the MSE between the estimated q-value\n",
    "function and the true optimal $q_{*}$ as a function of number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def qlearning(mdp, behavioral_policy, num_simulations=30, alpha=0.01, epsilon=0.1):\n",
    "\t# Conditions for convergence; note that we can also run on continuing problems\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\n",
    "\tQ = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\tQs = []\n",
    "\n",
    "\tfor t in range(num_simulations):\n",
    "\t\ts = mdp.reset()\n",
    "\t\twhile not mdp.is_terminal(s):\n",
    "\t\t\ta = mdp.sample_action(s, behavioral_policy)\n",
    "\t\t\t(s1, r) = mdp.step(s, a)\n",
    "\n",
    "\t\t\t# TODO: complete\n",
    "\n",
    "\t\tQs.append(copy.copy(Q))\n",
    "\t# determine policy from Q function\n",
    "\ttarget_policy = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\tfor state in mdp.terminal_states():\n",
    "\t\ttarget_policy[state,:] = 1.0 / mdp.n_actions\n",
    "\tfor state in mdp.nonterminal_states():\n",
    "\t\ta_max = np.random.choice(np.flatnonzero(Q[state] == np.max(Q[state])))\n",
    "\t\ttarget_policy[state, a_max] = 1.0\n",
    "\n",
    "\treturn target_policy, Q, Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "b_policy = uniform_stochastic_policy(mdpe.n_states, mdpe.n_actions)\n",
    "policy_Q, Q, Qs = qlearning(mdpe, behavioral_policy=b_policy, num_simulations=10000, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
