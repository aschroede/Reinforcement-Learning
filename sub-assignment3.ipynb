{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-assignment 3:  Monte Carlo Methods *(2 points)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter Notebook explores the implementation of Monte Carlo methods for solving Markov Decision Processes (MDPs). \n",
    "\n",
    "In this notebook, we will focus on the episodic version of MDPs, where the agent interacts with the environment in discrete episodes. We will start by defining a simple MDP called CircleWorld, which has a finite number of states and actions. \n",
    "\n",
    "We will then implement Monte Carlo methods for both prediction and control. For prediction, we will estimate the state-value function for the optimal policy using an on-policy, first-visit Monte Carlo algorithm. For control, we will optimize the policy using an on-policy, first-visit Monte Carlo algorithm and epsilon-greedy action selection.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sub-assignment is built on top of the previous sub-assignments, so it is important to complete them before, and utilize defined components in this sub-assignment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use the *episodic* version of the MDP (`mdpe` instantiated below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleWorld():\n",
    "\t\"\"\"\n",
    "\tGeneric base class for MDPs with finite state, action and reward spaces\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, n_states, n_actions, reward, task, gamma=1.0, state_labels=None, action_labels=None):\n",
    "\t\t\"\"\"\n",
    "\t\tn_states: number of states [0,...,N-1]\n",
    "\t\tn_actions: number of actions [0,...,N-1]\n",
    "\t\treward: reward values\n",
    "\t\ttask: episodic or continuing\n",
    "\t\tgamma: discounting factor\n",
    "\t\t\"\"\"\n",
    "\t\tself.n_states = n_states\n",
    "\t\tself.n_actions = n_actions\n",
    "\t\tself.state_labels = state_labels or np.arange(self.n_states)\n",
    "\t\tself.action_labels = action_labels or np.arange(self.n_actions)\n",
    "\t\tself.n_rewards = len(reward)\n",
    "\t\tself.reward = reward\n",
    "\t\tassert(task == 'episodic' or task == 'continuing')\n",
    "\t\tself.task = task\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\t\"\"\"\n",
    "\t\tSample initial state at start of the episode; assumed uniform\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.random.randint(self.n_states)\n",
    "\n",
    "\tdef p_transition(self, s, a, s1, r):\n",
    "\t\t\"\"\"\n",
    "\t\tTransition density s x a => s1 x r\n",
    "\t\t\"\"\"\n",
    "\t\t# This only works for deterministic state transitions; otherwise override\n",
    "\t\treturn np.float64((s1, r) == self.step(s, a))\n",
    "\n",
    "\tdef step(self, s, a):\n",
    "\t\t\"\"\"\n",
    "\t\tSample new state and reward when starting in s and taking action a\n",
    "\t\t:return new_state, reward_index: note that there are 3\n",
    "\t\t\"\"\"\n",
    "\t\ts1 = (s + 2 * a - 1) % self.n_states  # takes one step left or right\n",
    "\t\tif s1 == 0:\n",
    "\t\t\tr = 0\n",
    "\t\telse:\n",
    "\t\t\tr = 1\n",
    "\t\treturn s1, r\n",
    "\n",
    "\tdef sample_action(self, state, policy):\n",
    "\t\t# sample action from policy for a given state\n",
    "\t\treturn np.random.choice(np.arange(self.n_actions), p=policy[state])\n",
    "\n",
    "\tdef sample_episode(self, policy, T=None):\n",
    "\t\t\"\"\"\n",
    "\t\tSample a finite horizon sequence from an MDP using some policy\n",
    "\t\tIf the tasks is continuing then we sample exactly T steps\n",
    "\t\tIf the task is episodic then we sample exactly one episode or reset until we sample T steps\n",
    "\t\t\"\"\"\n",
    "\t\t# sequence element is state, action, reward\n",
    "\t\tseq = []\n",
    "\t\t# randomly sample initial state NOTE: For exploring starts we would need to sample both states and actions. This is not needed for epsilon-greedy policies\n",
    "\t\ts = self.reset()\n",
    "\n",
    "\t\tif self.task == 'continuing':\n",
    "\t\t\tassert (T is not None)\n",
    "\t\t\tfor t in range(T):\n",
    "\t\t\t\ta = self.sample_action(s, policy)\n",
    "\t\t\t\t(s1, r) = self.step(s, a)\n",
    "\t\t\t\tseq.append([s, a, self.reward[r]])\n",
    "\t\t\t\ts = s1\n",
    "\t\telse:\n",
    "\t\t\tt = 0\n",
    "\t\t\twhile True:\n",
    "\n",
    "\t\t\t\tif T is None and self.is_terminal(s):\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\telif t == T:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\ta = self.sample_action(s, policy)\n",
    "\t\t\t\tif self.is_terminal(s):\n",
    "\t\t\t\t\ts1 = self.reset()\n",
    "\t\t\t\t\tr = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t(s1, r) = self.step(s, a)\n",
    "\t\t\t\tseq.append([s, a, self.reward[r]])\n",
    "\t\t\t\ts = s1\n",
    "\t\t\t\tt = t+1\n",
    "\t\treturn seq\n",
    "\n",
    "\tdef optimal_policy(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFor this simple task, the optimal policy is just to move to state zero as quickly as possible\n",
    "\t\tand then flip back and forth\n",
    "\t\t\"\"\"\n",
    "\t\tpolicy = np.zeros([self.n_states, self.n_actions])\n",
    "\t\tn = int(np.round(self.n_states/2))\n",
    "\t\tpolicy[:n, 0] = 1.0\n",
    "\t\tpolicy[n:, 1] = 1.0\n",
    "\t\tpolicy[self.terminal_states(), :] = 1.0 / self.n_actions\n",
    "\t\treturn policy\n",
    "\n",
    "\tdef random_deterministic_policy(self):\n",
    "\t\t\"\"\"\n",
    "\t\tRandom choice of a deterministic action for each state\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.random.multinomial(1, [1.0 / self.n_actions for a in range(self.n_actions)], self.n_states).astype('float32')\n",
    "\n",
    "\tdef nonterminal_states(self):\n",
    "\t\t\"\"\"\n",
    "\t\t:return: The set S of nonterminal states\n",
    "\t\t\"\"\"\n",
    "\t\treturn [s for s in range(self.n_states) if not self.is_terminal(s)]\n",
    "\n",
    "\tdef terminal_states(self):\n",
    "\t\t\"\"\"\n",
    "\t\t:return: The set S of terminal states\n",
    "\t\t\"\"\"\n",
    "\t\treturn [s for s in range(self.n_states) if self.is_terminal(s)]\n",
    "\n",
    "\tdef is_terminal(self, s):\n",
    "\t\t\"\"\"\n",
    "\t\tflags if s is a terminal state\n",
    "\t\t\"\"\"\n",
    "\t\tif self.task == \"episodic\":\n",
    "\t\t\treturn s == 0\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tdef q_to_v(self, q, policy):\n",
    "\t\t\"\"\"\n",
    "\t\tConvert Q-value to state value\n",
    "\t\t:param q:\n",
    "\t\t:param policy:\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tv = np.zeros(self.n_states)\n",
    "\t\tfor s in self.nonterminal_states():\n",
    "\t\t\tfor a in range(self.n_actions):\n",
    "\t\t\t\tv[s] += policy[s, a] * q[s, a]\n",
    "\t\treturn v\n",
    "\n",
    "\tdef v_to_q(self, v):\n",
    "\t\t\"\"\"\n",
    "\t\tConvert state value to Q-value\n",
    "\t\t:param v:\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tq = np.zeros([self.n_states, self.n_actions])\n",
    "\t\tfor s in self.nonterminal_states():\n",
    "\t\t\tfor a in range(self.n_actions):\n",
    "\t\t\t\tq[s, a] = sum(self.p_transition(s, a, s1, r) * (self.reward[r] + self.gamma * v[s1])\n",
    "\t\t\t\t              for s1 in range(self.n_states) for r in range(self.n_rewards))\n",
    "\t\treturn q\n",
    "\n",
    "\t# some simple rendering methods\n",
    "\tdef __str__(self):\n",
    "\t\tstr = \"task: {0}\\n\".format(self.task)\n",
    "\t\tstr += \"states: {0}\\n\".format(self.state_labels)\n",
    "\t\tif self.terminal_states():\n",
    "\t\t\tstr += \"terminal states: {0}\\n\".format(self.terminal_states())\n",
    "\t\tstr += \"actions: {0}\\n\".format(self.action_labels)\n",
    "\t\tstr += \"rewards: {0}\\n\".format(self.reward)\n",
    "\t\tstr += \"discounting factor: {0}\".format(self.gamma)\n",
    "\t\treturn str\n",
    "\n",
    "\tdef print_policy(self, policy):\n",
    "\t\tfor s in range(self.n_states):\n",
    "\t\t\ta = np.random.choice(np.arange(self.n_actions), p=policy[s])\n",
    "\t\t\tprint('state ' + str(self.state_labels[s]) + ' => action ' + str(self.action_labels[a]))\n",
    "\n",
    "\tdef print_value(self, vf):\n",
    "\t\t\"\"\"\n",
    "\t\t:param vf: state value or action value function\n",
    "\t\t\"\"\"\n",
    "\t\tif vf.ndim == 1:\n",
    "\t\t\tfor s in range(self.n_states):\n",
    "\t\t\t\tprint('state ' + str(self.state_labels[s]) + ': ' + str(vf[s]))\n",
    "\t\telse:\n",
    "\t\t\tfor s in range(self.n_states):\n",
    "\t\t\t\tfor a in range(self.n_actions):\n",
    "\t\t\t\t\tprint('state ' + str(self.state_labels[s]) + ' - action ' + str(self.action_labels[a] + ': ' + str(vf[s,a])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: episodic\n",
      "states: [0 1 2 3 4 5 6 7 8 9]\n",
      "terminal states: [0]\n",
      "actions: ['L', 'R']\n",
      "rewards: [1.0, -0.1111111111111111]\n",
      "discounting factor: 1.0\n"
     ]
    }
   ],
   "source": [
    "mdpe = CircleWorld(n_states=10, n_actions=2, reward=[1.0, -1.0 /(10 - 1)], task='episodic', gamma=1.,\n",
    "                   state_labels=None, action_labels=['L', 'R'])\n",
    "print(mdpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a**) Why do we need to use an *episodic* task? (0.2 point)\n",
    "\n",
    "We use an episodic task to ensure that well defined returns are available - value estimates and policies are only updated after the completion of a full episode. This allows Monte Carlo methods to update in an episode-by-episode manner, but not in a step-by-step (online) manner. If a continuous task were used it would be difficult to decide when the value function and policy should be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**) **MC Prediction** (0.9 point): Implement an *on-policy, first-visit MC* algorithm to estimate the state-value function for\n",
    "the optimal policy\n",
    ". This should be provided as a function `mc_prediction` that takes in the initial policy and\n",
    " a parameter specifying how many sample episodes to run. Use your implementation to calculate the approximate state\n",
    " value function $V_{\\pi}$ for the optimal policy $\\pi_{*}$. Plot the MSE between the estimated value function and the\n",
    "  true optimal $v_{*}$ as a function of number of episodes, i.e. how the accuracy of the estimate evolves across\n",
    "  episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reversed_first_visits(episode):\n",
    "\tfirst_visits = {}\n",
    "\tvisited_states = set()\n",
    "\n",
    "\tfor index, step in enumerate(episode):\n",
    "\t\tstate, action, reward = step\n",
    "\t\tif state not in visited_states:\n",
    "\t\t\tvisited_states.add(state)\n",
    "\t\t\tfirst_visits[state] = len(episode)- 1 - index\n",
    "\n",
    "\treturn first_visits\n",
    "\n",
    "\n",
    "\n",
    "def mc_prediction(mdp: CircleWorld, policy, num_simulations=30):\n",
    "\t# Conditions for convergence\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\t\n",
    "\n",
    "\tVs = []\n",
    "\tV = [0 for i in range(10)]\n",
    "\tReturns = [0 for i in range(10)]\n",
    "\tCounts= [0 for i in range(10)]\n",
    "\n",
    "\t# Generating num_simulations episodes\n",
    "\tfor i in range(num_simulations):\n",
    "\n",
    "\t\t# Generate new episode\n",
    "\t\tepisode = mdp.sample_episode(policy)\n",
    "\n",
    "\t\t# Determine first-visit timestep for each state\n",
    "\t\tfirst_visits = find_reversed_first_visits(episode)\n",
    "\n",
    "\t\tG = 0\n",
    "\t\t\n",
    "\t\t# Iterating over steps in an episode\n",
    "\t\tfor timestep, step in enumerate(reversed(episode)):\n",
    "\t\t\tstate, action, reward = step\n",
    "\n",
    "\t\t\tG = G*mdp.gamma + reward\n",
    "\n",
    "\t\t\t# If this is the first time state is observed in sequence, then add to Returns\n",
    "\t\t\tif timestep == first_visits[state]:\n",
    "\t\t\t\tReturns[state] += G\n",
    "\t\t\t\tCounts[state] += 1\n",
    "\t\t\t\tV[state] = Returns[state]/Counts[state]\n",
    "\n",
    "\t\tVs.append(V.copy())\n",
    "\n",
    "\treturn V, Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.0, 0.8888888888888892, 0.7777777777777777, 0.6666666666666665, 0.5555555555555554, 0.6666666666666665, 0.7777777777777777, 0.8888888888888892, 1.0]\n"
     ]
    }
   ],
   "source": [
    "policy = mdpe.optimal_policy()\n",
    "\n",
    "V, value_functions_mc = mc_prediction(mdpe, policy, num_simulations=30)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not converge\n",
      "[887.88888889 888.88888889 888.77777778 888.66666667 888.55555556\n",
      " 884.88888889 885.88888889 886.88888889 887.88888889 888.88888889]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'State value')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots\n",
    "# Somehow need to get the true value of the optimal policy to compare it with the value of the approximation from MC\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "pi_e = mdpe.optimal_policy()\n",
    "\n",
    "# True values seem to be messed up here... not sure why the \n",
    "V_e = policy_evaluation(mdpe, pi_e, max_t=1000)\n",
    "print(V_e)\n",
    "\n",
    "time_axis_pi = np.arange(len(value_functions_mc))\n",
    "print(time_axis_pi)\n",
    "ax.plot(time_axis_pi, list(map(lambda x: np.mean((x - V_e)**2), value_functions_mc)), '-o', label='policy iteration')\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('State value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**) **MC Control** (0.9 point): Implement an *on-policy, first-visit MC* algorithm to optimize the policy. To do this, you will\n",
    "need to\n",
    "estimate the Q-value function (computing returns after first visit to state-action pairs) and use it to take an\n",
    "$\\epsilon$-greedy action selection to gradually improve the policy. This should be provided as a function\n",
    "`mc_control` that takes in the initial policy, the number of episodes to use and `epsilon` to set the mininum action\n",
    "probability. Run for 200 episodes and plot the MSE between the estimated q-value function and the\n",
    "  true optimal $q_{*}$ as a function of number of episodes. Note: you can obtain the $q_{*}$ from\n",
    "$v_{*}$ using the method `mdpe.v_to_q()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(mdp, policy, num_simulations=30, epsilon=0.01):\n",
    "\t\"\"\"\n",
    "\tRun the Monte Carlo First Visit On-Policy algorithm and return the estimated\n",
    "\tpolicy, Q (state action) values, and returns (rewards) dict.\n",
    "\tUses epsilon-soft policies instead of exploring starts\n",
    "\t:param mdp:\n",
    "\t:param policy: any epsilon soft policy; e.g. mdp.uniform_stochastic_policy()\n",
    "\t:param num_simulations : int Number of episodes for the policy iteration process\n",
    "\t:param epsilon: epsilon-soft minimum probability\n",
    "\n",
    "\t:return policy: numpy.ndarray Estimated Policy\n",
    "\t:return Q: numpy.ndarray Estimated Q (state-action) values\n",
    "\t:return R: dict Returns obtained for every state\n",
    "\t\"\"\"\n",
    "\t# Conditions for convergence\n",
    "\tassert(mdp.task == 'episodic')\n",
    "\n",
    "\tQ = np.zeros([mdp.n_states, mdp.n_actions])\n",
    "\n",
    "\t# TODO\n",
    "\n",
    "\treturn policy, Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare with uniform stochastic policy in the previous part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniform_stochastic_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23668/2449186772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniform_stochastic_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpolicy_mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQs_mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_simulations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uniform_stochastic_policy' is not defined"
     ]
    }
   ],
   "source": [
    "policy0 = uniform_stochastic_policy(mdpe.n_states, mdpe.n_actions)\n",
    "policy_mc, Qs_mc = mc_control(mdpe, policy0, num_simulations=10000, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
