{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Reinforcement Learning 3: *Dynamic Programming*\n",
    "\n",
    "This week's workgroups contain two parts: \n",
    "1. **Hand-written questions**, which are of similar difficulty as those you will encounter in the exam. Use this opportunity to exercise and clarify your doubts with the course's TAs.\n",
    "2. **Programming  exercises**, which will enable you get hands-on experience on Dynamic Programming algorithms. It will be very helpful for you to complete the assignment that will be given to you after the workgroups. \n",
    "\n",
    "***Note:*** Immediately after the workgroup, you will receive a programming assignment that constitutes a segment of the  final assignment (the one due at the end of this course). The practical skills acquired during the workgroup sessions are designed to equip you with the necessary tools to successfully tackle this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refreshing your memory: elements from the previous lectures\n",
    "We use a Markov Decision Process (MDP) to formulize a RL problem, which is described by the tuple \n",
    "$$\\mathcal{M}=\\langle\\mathcal{S},  {\\mathcal{A}}, \\mathcal{P}, \\mathcal{R},\\gamma\\rangle,$$\n",
    "where $\\mathcal S$ is the state space, $\\mathcal A$ is the action space, and $\\mathcal R$ is the reward space. In this course, we assume that $\\mathcal S, \\mathcal A, \\mathcal R$ are all finite sets. Here, $\\mathcal{P}$ defines the dynamics of the MDP, characterizing the state-transition probabilities and reward-generating probabilities. $\\gamma\\in[0,1]$ is the discount factor that trades off later rewards to earlier ones\n",
    "\n",
    "### **Value functions**: total amount of reward an agent can expect to accumulate over the future, starting from that state/state-action pair.\n",
    "\n",
    "`State value funcitons:` (expected return, when stating in $s$ and following policy $\\pi$)\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right].$$\n",
    "`(State-)Action value funcitons:` (expected return, when stating in $s$, taking action $a$ and following policy $\\pi$)\n",
    "$$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right].$$\n",
    "\n",
    "### **Bellman equations**\n",
    "`For state value:` (connecting the value of a state $s$ to the value of the successor states $s'$ and immediate rewards $r$)\n",
    "\\begin{align}\n",
    "v(s) &\\doteq \\mathbb{E}\\left[G_{t} \\mid S_{t}=s\\right]\\nonumber\\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right]\\nonumber\\\\\n",
    "&{\\color{red}=\\sum\\limits_{a} \\pi(a \\mid s) \\sum\\limits_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]}, \\quad \\text { for all } s \\in \\mathcal{S} \n",
    "\\end{align}\n",
    "\n",
    "`For action value:` (connecting the value of a state-action pair $(s,a)$ to the value of the successor state-action pairs $(s', a')$ and immediate rewards $r$)\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) &\\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right]\\\\\n",
    "& = \\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s, A_{t}=a\\right]\\\\\n",
    "& \\color{red}=\\sum\\limits_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\sum\\limits_{a'} \\pi(a'\\mid s')q_{\\pi}(s', a')\\right], \\quad \\text { for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}\n",
    "\\end{align*}\n",
    "`Connections between state and action value functions:` \n",
    "\\begin{align*}\n",
    "v_{\\pi}(s)&=\\mathbb{E}_{\\pi}[q_{\\pi}(s,a)]=\\sum\\limits_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\ \\  q_{\\pi}(s, a)\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s,a)&=\\mathbb{E}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1}) \\mid S_{t}=s,A_{t}=a]\\\\\n",
    "&=\\sum\\limits_{s',r} p(s',r \\mid s,a) [r+\\gamma v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "***Note:*** *The relations above can be useful when evaluating the value function of a policy.*\n",
    "\n",
    "### **Bellman optimality equations**\n",
    "There exists a policy that is better than or equal to all other policies. This policy is called an ***optimal policy***.  There can be more than one optimal policies, and we denote them as $\\pi_*$. They share the same value function, which we call optimal state value function and state-action value function, respectively defined as \n",
    "\\begin{align*}\n",
    "&v_*(s)=\\max_\\pi v_\\pi(s), \\text{ for all } s \\in \\mathcal S;\\\\\n",
    "& q_*(s,a) =\\max_\\pi q_\\pi(s,a), \\text{ for all } s \\in \\mathcal S, a \\in \\mathcal A.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "`For state value,` the Bellman optimality equation read as $$v_{*}(s)=\\max \\limits_{a} \\sum\\limits_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right].$$\n",
    "Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.\n",
    "\n",
    "\n",
    "`For action value,` the Bellman optimality equation read as\n",
    "\\begin{align*}\n",
    "q_{*}(s,a)=\\sum\\limits_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\max\\limits _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\n",
    "\\end{align*}\n",
    "\n",
    "### **Policy  evaluation**\n",
    "\n",
    "Given a policy $\\pi$, to evaluate the value function, we can use the Bellman equation in `Eq.(1)` iteratively:\n",
    "\n",
    "\\begin{align*}\n",
    "v_{\\color{red}k+1}({\\color{blue}s}) &= \\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma v_{\\color{red}k}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right]\\\\\n",
    "&=\\sum\\limits_{a} \\pi(a \\mid s) \\sum\\limits_{{\\color{blue}s'}, r} p\\left({\\color{blue}s'}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\color{red} k}\\left({\\color{blue}s'}\\right)\\right]\n",
    "\\end{align*}\n",
    "where $k=0,1,2, \\dots$ As $k\\to \\infty$, $v_{k}$ will converge to the true value of $\\pi$.\n",
    "\n",
    "### **Policy  improvement**\n",
    "Once the value of a policy has been well evaluated, we can improve it in this way: at all states, we select at each state the action that appears best according to $q_\\pi(s,a)$. That is, consider the new greedy policy $\\pi'$ given by\n",
    "\\begin{align*}\n",
    "\\pi^{\\prime}(s) &\\doteq \\underset{a}{\\arg \\max } \\ \\ q_{\\pi}(s, a)\\\\\n",
    "&=\\underset{a}{\\arg \\max } \\  \\sum\\limits_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right].\n",
    "\\end{align*}\n",
    "\n",
    "**Policy iteration:** *By alternating between policy evaluation and policy improvement, one can find the optimal policy.*\n",
    "\n",
    "Check the slides and the textbook for `value interation` and `asynchronous dynamical programming`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## 1. Grid World\n",
    "\n",
    "\n",
    "Recall the GridWorld example covered in the course lecture:\n",
    "\n",
    "![Grid World](https://raw.githubusercontent.com/yuzhenqin90/RLcourse/main/WG3/Lec3-grid-world.png \"Grid World Example\")\n",
    "\n",
    "\n",
    "**References:**\n",
    "- Sutton & Barto, Ch.4, p. 76 (example 4.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's define the GridWorld MDP\n",
    "class GridWorld:\n",
    "    # example 1 in DP\n",
    "    def __init__(self, grid_size, actions):\n",
    "        self.grid_size = grid_size\n",
    "        self.actions = actions\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        x, y = state\n",
    "        return (x == 0 and y == 0) or (x == self.grid_size - 1 and y == self.grid_size - 1)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        if self.is_terminal(state):\n",
    "            return state, 0\n",
    "        next_state = (np.array(state) + action).tolist()\n",
    "        x, y = next_state\n",
    "        if x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size:\n",
    "            next_state = state\n",
    "        reward = -1\n",
    "        return next_state, reward\n",
    "\n",
    "    @staticmethod\n",
    "    def draw(image):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_axis_off()\n",
    "        tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "        nrows, ncols = image.shape\n",
    "        width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "        # Add cells\n",
    "        for (i, j), val in np.ndenumerate(image):\n",
    "            tb.add_cell(i, j, width, height, text=val,\n",
    "                        loc='center', facecolor='white')\n",
    "\n",
    "            # Row and column labels...\n",
    "        for i in range(len(image)):\n",
    "            tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
    "                        edgecolor='none', facecolor='none')\n",
    "            tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
    "                        edgecolor='none', facecolor='none')\n",
    "        ax.add_table(tb)\n",
    "\n",
    "    def compute_state_value(self, action_prob, in_place=True, discount=1.0):\n",
    "        new_state_values = np.zeros((self.grid_size, self.grid_size))\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            if in_place:\n",
    "                state_values = new_state_values\n",
    "            else:\n",
    "                state_values = new_state_values.copy()\n",
    "            old_state_values = state_values.copy()\n",
    "\n",
    "            for i in range(self.grid_size):\n",
    "                for j in range(self.grid_size):\n",
    "                    value = 0\n",
    "                    for action in self.actions:\n",
    "                        (next_i, next_j), reward = self.step([i, j], action)\n",
    "                        value += action_prob * (reward + discount * state_values[next_i, next_j])\n",
    "                    new_state_values[i, j] = value\n",
    "            max_delta_value = abs(old_state_values - new_state_values).max()\n",
    "            if max_delta_value < 1e-4:\n",
    "                break\n",
    "            iteration += 1\n",
    "        return new_state_values, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grid_size = 4\n",
    "# left, up, right, down\n",
    "actions = [np.array([0, -1]),\n",
    "           np.array([-1, 0]),\n",
    "           np.array([0, 1]),\n",
    "           np.array([1, 0])]\n",
    "policy = 0.25\n",
    "\n",
    "# instantiate the GridWorld\n",
    "gw = GridWorld(grid_size=grid_size, actions=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**a**) Evaluate the state-value function, using synchronous and asynchronous (in-place) updates. How many iterations do the algorithms take to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async_values, async_iteration = gw.compute_state_value(action_prob=policy, in_place=True)\n",
    "gw.draw(np.round(async_values, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sync_values, sync_iteration = gw.compute_state_value(action_prob=policy, in_place=False)\n",
    "gw.draw(np.round(sync_values, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-place: 113 iterations\n",
      "Synchronous: 172 iterations\n"
     ]
    }
   ],
   "source": [
    "print('In-place: {} iterations'.format(async_iteration))\n",
    "print('Synchronous: {} iterations'.format(sync_iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## **b**) **Pen+Paper exercises** (in all exercises below, assume $\\gamma=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**b.1**) If the initial policy $\\pi$ is the equiprobable random policy, i.e. $\\pi(a\\mid s)=0.25,\n",
    "\\forall a\n",
    "\\in \\mathcal{A}, s\\in\\mathcal{S}$, calculate the value of being in state 11 and taking action $\\texttt{down}$, $q_{\\pi}\n",
    "(11, \\texttt{down})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Remember from the lecture that $$q_{\\pi}(s, a) =\\sum\\limits_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma\n",
    "v_{\\pi}(s)\\right]$$\n",
    " Then $$q_{\\pi}(11, \\texttt{down}) = -1 + v_{\\pi}(T) = -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**b.2**) What is the value of being in state 7 and taking action $\\texttt{down}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "$$q_{\\pi}(7, \\texttt{down}) = -1 + v_{\\pi}(11) = -1 + (-14) = -15$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**b.3**) Suppose a new state is added to the gridworld below state 13 and the actions that can be taken from that\n",
    "state are the same as from any other state and take the agent to: $a = \\texttt{up} \\rightarrow s'=13$, $a =\n",
    "\\texttt{left} \\rightarrow\n",
    "s'=12$, $a = \\texttt{right} \\rightarrow s'=14$, $a = \\texttt{down} \\rightarrow s'=15$. Assume that all transitions from the\n",
    "original states\n",
    " are unchanged. What is $v_{\\pi}(15)$ for the equiprobable random policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Remember from the lecture that\n",
    "\n",
    "$$v_{\\pi}(s)=\\sum\\limits_{a} \\pi(a \\mid s) \\sum\\limits_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\n",
    "\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$v_{\\pi}(15) = 0.25 \\times (-1 + v_{\\pi}(13)) + 0.25 \\times (-1 + v_{\\pi}(12)) + 0.25 \\times (-1 + v_{\\pi}(14)) + 0.25 \\times (-1 + v_{\\pi}(15)) \\\\ = -15 + 0.25 v_{\\pi}(15) \\Leftrightarrow \\\\ v_{\\pi}\n",
    "(15)=-20$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**b.4**) Now suppose the dynamics for state 13 are also changed by the existence of the new state 15. Now, being in\n",
    "state 13 and moving $\\texttt{down}$ takes the agent to state 15. What is the value of state 15, $v_{\\pi}(15)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This change to the dynamics does not affect the value of state 15, so $v_{\\pi}(15) = -20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**b.5**) Under the changes done in b.4, what is the value of state 13, $v_{\\pi}(13)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the original MDP, moving $\\texttt{down}$ from state 13 would keep the agent in state 13, because it is a boundary\n",
    "state, i.e. $s'=13$. If we modify the dynamics such that the target state now becomes 15, $s'=15$, the value of state\n",
    " 13 is unchanged because $v_{\\pi}(13) = v_{\\pi}(15) = -20$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**c**) **Challenge** Re-implement the GridWorld as a `gym` environment and perform synchronous and asynchronous value iteration. If you feel adventurous, you can try to add proper rendering to show the position of the agent in the grid-world.\n",
    "\n",
    "***Note:*** This is optional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. Frozen Lake (programming exercise)\n",
    "\n",
    "**Note:** In the exercises below, you are not given the complete implementations, but instead should try to solve the\n",
    "exercises. The scaffold is in place and all you need to do is fill in the #TODO fields.\n",
    "The solutions will be posted on Brightspace after the workgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The Frozen Lake environment is a 4×4 grid which contain four possible areas  — Safe (S), Frozen (F), Hole (H) and Goal (G). The agent moves around the grid until it reaches the goal or the hole. The agent in the environment has four possible moves — Up, Down, Left and Right. If it falls into the hole, it has to start from the beginning and is rewarded the value 0.\n",
    "The process continues until it learns from every mistake and reaches the goal. Here is a visual description of the Frozen Lake grid task:\n",
    "\n",
    "![](https://raw.githubusercontent.com/yuzhenqin90/RLcourse/main/WG3/FrozenLake.png)\n",
    "\n",
    "Note that the ice is slippery, so the agent won't always move in the direction intended by the action. Specifically, there is a 1/3 chance of moving in the direction prescribed by the action and 1/3 to each orthogonal direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_episodes(environment, n_episodes, policy, display=True):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for episode in range(n_episodes):\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "        while not terminated:\n",
    "            # Select an action to perform in a current state (a > 0).all():\n",
    "            if policy == 'random':\n",
    "                action = environment.action_space.sample()\n",
    "            else:\n",
    "                if isinstance(state, tuple):\n",
    "                    state = state[0]\n",
    "                action = np.argmax(policy[state])\n",
    "\n",
    "            # Perform an action and observe how environment acted in response\n",
    "            next_state, reward, terminated, truncated, info = environment.step(action)\n",
    "\n",
    "            # Plot the first episode\n",
    "            # if episode==1 and display:\n",
    "            #         print(\"Action:\")\n",
    "            #         environment.render() # display current agent state\n",
    "            # Summarize total reward\n",
    "            total_reward += reward\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "            # Calculate number of wins over episodes\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "    average_reward = total_reward / n_episodes\n",
    "    return wins, total_reward, average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First episode:\n",
      "------------------------------------\n",
      "Summary:\n",
      "- number of wins over 5000 episodes = 60\n",
      "- average reward over 5000 episodes = 0.012\n"
     ]
    }
   ],
   "source": [
    "# Load a Frozen Lake environment\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\")\n",
    "# Number of episodes to play\n",
    "n_episodes = 5000\n",
    "# First episode plotted as a sample episode\n",
    "print('First episode:')\n",
    "wins, total_reward, average_reward = run_episodes(env, n_episodes, policy=\"random\")\n",
    "print('------------------------------------')\n",
    "print('Summary:')\n",
    "print(f'- number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'- average reward over {n_episodes} episodes = {average_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## **a**) Implement the Iterative Policy Evaluation algorithm as a function to evaluate the given policy. How many iterations does the random policy need to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Number of evaluation iterations\n",
    "    evaluation_iterations = 1\n",
    "    # Initialize a value function for each state as zero\n",
    "    V = np.zeros(environment.observation_space.n)\n",
    "    # Repeat until change in value is below the threshold\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Initialize a change of value function as zero\n",
    "        delta = 0\n",
    "        # Iterate through each state\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # Initial a new value of current state\n",
    "            v = 0\n",
    "            # Try all possible actions which can be taken from this state\n",
    "            for action, action_probability in enumerate(policy[state]):\n",
    "                # Check how good next state will be\n",
    "                for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            # Calculate the absolute change of value function\n",
    "            delta = max(delta, np.abs(V[state] - v))\n",
    "            # Update value function\n",
    "            V[state] = v\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # Terminate if value change is insignificant\n",
    "        if delta < theta:\n",
    "            print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "            return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 66 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuzhen.qin/anaconda3/envs/RLcourse/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01393979, 0.01163093, 0.02095298, 0.01047649, 0.01624866,\n",
       "       0.        , 0.04075154, 0.        , 0.0348062 , 0.08816993,\n",
       "       0.14205316, 0.        , 0.        , 0.17582037, 0.43929118,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = {s: [1./env.action_space.n for _ in range(env.action_space.n)] for s in range(env.observation_space.n)}\n",
    "policy_evaluation(policy=policy, environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## **b**) Using your `Policy Evaluation` function from (a), implement the Policy iteration algorithm. Run the Policy iteration to obtain the optimal policy for the `FrozenLake-v1` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "    action_values = np.zeros(environment.action_space.n)\n",
    "    for action in range(environment.action_space.n):\n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    # Start with a random policy\n",
    "    #num states x num actions / num actions\n",
    "    policy = {s: [1./env.action_space.n for _ in range(env.action_space.n)] for s in range(env.observation_space.n)}\n",
    "    # Initialize counter of evaluated policies\n",
    "    evaluated_policies = 1\n",
    "    # Repeat until convergence or critical number of iterations reached\n",
    "    for i in range(int(max_iterations)):\n",
    "        stable_policy = True\n",
    "        # Evaluate current policy\n",
    "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
    "        # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # Choose the best action in a current state under current policy\n",
    "            current_action = np.argmax(policy[state])\n",
    "            # Look one step ahead and evaluate if current action is optimal\n",
    "            # We will try every possible action in a current state\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select a better action\n",
    "            best_action = np.argmax(action_value)\n",
    "            # If action didn't change\n",
    "            if current_action != best_action:\n",
    "                stable_policy = False\n",
    "                # Greedy policy update\n",
    "                policy[state] = np.eye(environment.action_space.n)[best_action]\n",
    "        evaluated_policies += 1\n",
    "        # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "        if stable_policy:\n",
    "            # print(policy)\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 66 iterations.\n",
      "Policy evaluated in 170 iterations.\n",
      "Policy evaluated in 428 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration: number of wins over 1000 episodes = 839\n",
      "Policy iteration: average reward over 1000 episodes = 0.839 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 1000\n",
    "iteration_name = \"Policy iteration\"\n",
    "iteration_func = policy_iteration\n",
    "# Load a Frozen Lake environment\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\")\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = iteration_func(env)\n",
    "# Apply the best policy to the real environment\n",
    "wins, total_reward, average_reward = run_episodes(env, n_episodes, policy)\n",
    "print(f'{iteration_name}: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{iteration_name}: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **c**) Implement the `Value iteration` algorithm. Run the algorithm to obtain the optimal policy for the `FrozenLake-v1` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Initialize state-value function with zeros for each environment state\n",
    "    V = np.zeros(environment.observation_space.n)\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Early stopping condition\n",
    "        delta = 0\n",
    "        # Update each state\n",
    "        for state in range(environment.observation_space.n):\n",
    "            # Do a one-step lookahead to calculate state-action values\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select best action to perform based on the highest state-action value\n",
    "            best_action_value = np.max(action_value)\n",
    "            # Calculate change in value\n",
    "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "            # Update the value function for current state\n",
    "            V[state] = best_action_value\n",
    "            # Check if we can stop\n",
    "        if delta < theta:\n",
    "            print(f'Value-iteration converged at iteration #{i}.')\n",
    "            break\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    # policy = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
    "    policy = {s: [0 for _ in range(env.action_space.n)] for s in range(env.observation_space.n)}\n",
    "    for state in range(environment.observation_space.n):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "        # Select best action based on the highest state-action value\n",
    "        best_action = np.argmax(action_value)\n",
    "        # Update the policy to perform a better action at a current state\n",
    "        policy[state][best_action] = 1.0\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration #523.\n",
      "Value iteration: number of wins over 1000 episodes = 817\n",
      "Value iteration: average reward over 1000 episodes = 0.817 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 1000\n",
    "iteration_name = \"Value iteration\"\n",
    "iteration_func = value_iteration\n",
    "# Load a Frozen Lake environment\n",
    "environment = gym.make('FrozenLake-v1', map_name=\"4x4\")\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = iteration_func(environment)\n",
    "# Apply the best policy to the real environment\n",
    "wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy)\n",
    "print(f'{iteration_name}: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{iteration_name}: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**d**) Compare two optimal policies in part (b) and (c). Which seems to converge faster and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 66 iterations.\n",
      "Policy evaluated in 170 iterations.\n",
      "Policy evaluated in 428 iterations.\n",
      "Policy Iteration :: number of wins over 5000 episodes = 4134\n",
      "Policy Iteration :: average reward over 5000 episodes = 0.8268 \n",
      "\n",
      "\n",
      "Value-iteration converged at iteration #523.\n",
      "Value Iteration :: number of wins over 5000 episodes = 4161\n",
      "Value Iteration :: average reward over 5000 episodes = 0.8322 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 5000\n",
    "# Functions to find best policy\n",
    "solvers = [('Policy Iteration', policy_iteration),\n",
    "           ('Value Iteration', value_iteration)]\n",
    "for iteration_name, iteration_func in solvers:\n",
    "    # Load a Frozen Lake environment\n",
    "    environment = gym.make(\"FrozenLake-v1\", map_name=\"4x4\")\n",
    "    # Search for an optimal policy using policy iteration\n",
    "    policy, V = iteration_func(environment.env)\n",
    "    # Apply the best policy to the real environment\n",
    "    wins, total_reward, average_reward = run_episodes(environment, n_episodes, policy, display=False)\n",
    "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
